{'n_layers': 2, 'units_0': 64, 'units_1': 32, 'activation': 'relu', 'batchnorm': True, 'dropout': 0.2, 'lr': 0.001, 'batch_size': 32, 'epochs': 80}